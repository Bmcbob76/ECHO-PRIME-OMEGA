# üéñÔ∏è GPU INFERENCE CONFIGURATION
# Authority Level: 11.0
# Location: E:\ECHO_XV4\MLS\servers\gpu_config.env

# =============================================================================
# GPU SERVER SETTINGS (RTX 4070 MACHINE)
# =============================================================================

# GPU Server IP Address - OMEN-40L (RTX 4090/High-End)
GPU_SERVER_HOST=192.168.1.187

# Ollama Port (default: 11434)
GPU_SERVER_PORT=11434

# Default Model to Use - QWEN FOR COMPLEX CODING
# Primary: Qwen2.5-Coder (large context, superior coding)
# Options:
#   - qwen2.5-coder:32b-instruct-q5_K_M (RECOMMENDED - elite coding)
#   - qwen2.5-coder:14b-instruct-q5_K_M (balanced)
#   - qwen2.5:32b-instruct-q5_K_M (general purpose)
#   - mixtral:8x7b-instruct-v0.1-q4_K_M (fallback)
GPU_MODEL=qwen2.5-coder:32b-instruct-q5_K_M

# Timeout for GPU requests (seconds)
GPU_TIMEOUT=120

# =============================================================================
# API SERVER SETTINGS (ECHO_XV4 MACHINE)
# =============================================================================

# API Server Port
API_SERVER_PORT=8070

# Enable Debug Mode
DEBUG_MODE=True

# =============================================================================
# RECOMMENDED MODELS FOR 16GB VRAM
# =============================================================================
# 
# GENERAL PURPOSE:
# - mixtral:8x7b-instruct-v0.1-q4_K_M (~26GB quantized to ~5GB)
# - qwen2.5:14b-instruct-q5_K_M (~14GB, excellent reasoning)
#
# CODE GENERATION:
# - deepseek-coder:33b-instruct-q4_K_M (~19GB quantized)
# - codellama:34b-instruct-q4_K_M (~19GB quantized)
# - starcoder2:15b-instruct-q5_K_M (~15GB)
#
# FAST INFERENCE:
# - llama3.2:11b-instruct-q5_K_M (~11GB, very fast)
# - phi3:14b-instruct-q5_K_M (~14GB, compact)
#
# Note: q4_K_M and q5_K_M are quantization levels
#   - q4_K_M: ~4-bit quantization (smaller, faster, less accurate)
#   - q5_K_M: ~5-bit quantization (larger, slower, more accurate)
#
# =============================================================================
